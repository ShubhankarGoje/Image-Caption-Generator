# Image-Caption-Generator
It was in 3rd grade that we were asked to describe an image vividly in words. Teachers asked us to use our “imagination” to not only see what is present in the image but what could be. The story behind it. While it could be an easy task for an adolescent, for a visual model it still is a challenge that needs to be overcome. It is difficult for computers to identify, separate, and characterize objects, situations, or behaviors that are impacted by numerous external circumstances, such as occlusions, lighting changes, and position variations. And we did just that, taught a computer to describe an image. 

## The Dataset
Flickr datasets of 8k and 30k were utilized for this project. Flickr 8k is made up of 8,000 photos, each accompanied by five distinct captions that give detailed explanations of the important entities and events produced by human annotators. The Flickr30k dataset includes 31,000 photos.  The photographs were hand-picked from six separate Flickr groups and do not feature any well-known persons or locations, but were chosen to illustrate a diversity of events and circumstances. Cleaning image data entailed deleting duplicate entities as well as low-quality photos and preprocessing for CNN. While cleaning text data, it was necessary to remove punctuation and capitalization.

## The CNN
To extract features from an input picture, a convolutional neural network (CNN) is employed. The preprocessed picture is fed into the CNN, which is made up of several layers, including convolutional layers, pooling layers, and fully-connected layers. The convolutional layers are in charge of identifying picture patterns such as edges and textures. The pooling layers limit the spatial resolution of the picture, making the CNN more resistant to tiny changes in the image. After passing through the CNN, the output is a feature map, which is a multi-dimensional array of values that represent the features of the image. The CNN utilized in the project is pre-trained and has some understanding of typical patterns and characteristics in images. The model may exploit this pre-existing information by employing a pre-trained CNN, which requires less training data to provide accurate results.

## The LSTM
To construct a textual description of the input image, Long Short-Term Memory (LSTM) is used. LSTM is a form of Recurrent Neural Network (RNN) that is used to simulate long-term dependencies and manage sequential input. The CNN feature map is sent into the LSTM, which processes it through successive layers of LSTM cells. The LSTM cells are intended to recall past information and utilize it to forecast future information. During training, the LSTM learns to associate the features of an image with the appropriate words and phrases to generate a textual description. The LSTM generates captions by predicting the next word in a sequence given the previous words. The LSTM starts with an initial word, and then iteratively generates the next word in the sequence, until the end of the sequence is reached. The final sequence of words generated by the LSTM is the textual description of the image.

## The Results
In comparison to the baseline in the dataset, we find that our model can produce captions that are more distinctive and novel (not seen in training data). Although the average length of our captions is less than the baseline, it is only roughly one word less when compared with the human-annotated captions. The vocabulary size used in our experiment was Flickr8k and the model performed well with a BLEU-1 score of 0.66 and BLEU-2 score of 0.51.
